<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves com- parable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.">
  <meta property="og:title" content="TableLlama" />
  <meta property="og:description"
    content="Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves com- parable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables." />
  <meta property="og:url" content="https://osu-nlp-group.github.io/TableLlama/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/TableLlama_transparent_background.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TableLlama: Towards Open Large Generalist Models for Tables">
  <meta name="twitter:description"
    content="Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves com- parable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/TableLlama_transparent_background.png">
  <meta name="twitter:card" content="static/images/overview1_yifei_1114-v13.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="Large Language Models, Instruction Tuning, Table Understanding, Natural Language Processing, Llama">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TableLlama: Towards Open Large Generalist Models for Tables</title>
  <link rel="icon" type="image/x-icon" href="static/images/TableLlama_transparent_background.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/OSU-NLP-Group">
          <span class="icon">
            <svg class="svg-inline--fa fa-home fa-w-18" aria-hidden="true" focusable="false" data-prefix="fas"
              data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg="">
              <path fill="currentColor"
                d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z">
              </path>
            </svg><!-- <i class="fas fa-home"></i> Font Awesome fontawesome.com -->
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
              MAmmoTH
            </a>
            <a class="navbar-item" href="https://osu-nlp-group.github.io/Mind2Web/">
              Mind2Web
            </a>
            <a class="navbar-item" href="https://osu-nlp-group.github.io/MagicBrush/">
              MagicBrush
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- Flex container for icon and title -->
            <div style="display: flex; align-items: center; justify-content: center;">
              <img src="static/images/TableLlama_transparent_background.png" alt="Icon"
                style="height: 7em; margin-right: -90px;">
              <h1 class="title is-1 publication-title">TableLlama: Towards Open Large Generalist Models for Tables</h1>
            </div>
            <!-- <h1 class="title is-1 publication-title">TableLlama: Towards Open Large Generalist Models for Tables</h1> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <sup>1</sup><a href="https://zhangtianshu.github.io/tianshu-zhang.github.io/" target="_blank">Tianshu
                  Zhang</a>,</span>
              <span class="author-block">
                <sup>2</sup><a href="https://xiangyue9607.github.io/" target="_blank">Xiang Yue</a>,</span>
              <span class="author-block">
                <sup>1</sup><a href="https://flyhero99.github.io" target="_blank">Yifei Li</a>,</span>
              <span class="author-block">
                <sup>1</sup><a href="https://web.cse.ohio-state.edu/~sun.397/" target="_blank">Huan Sun</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>The Ohio State University,
                <sup>2</sup>IN.AI Research
                <!-- <br>Conferance name and year</span> -->
                <br>
                <span class="author-block">
                  <a href="mailto:zhang.11535@osu.edu">zhang.11535@osu.edu</a>,
                  <a href="mailto:xiangyue@in.ai">xiangyue@in.ai</a>,
                  <a href="mailto:li.14042@osu.edu">li.14042@osu.edu</a>,
                  <a href="mailto:sun.397@osu.edu">sun.397@osu.edu</a>
                </span>
                <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Huggingface Dataset link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/osunlp/TableInstruct/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">🤗</span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- Huggingface Model link -->
                <span class="link-block">
                  <a href="https://huggingface.co/osunlp/TableLlama/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">🤗</span>
                    <span>Models</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/OSU-NLP-Group/TableLlama" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.09206.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h2 class="title is-3">Introduction</h2> -->
            <div class="content has-text-justified">
              <p>
                In this work, we proposed <b>TableLlama</b> and <b>TableInstruct</b>, which are (up to Nov, 2023) the
                first
                <b>open-source generalist model</b> and <b>dataset</b> for <b>tables</b>. Everything is
                <b>open-source</b> right now!
              </p>
              <p>
                <b>TableInstruct</b> is a large-scale instruction tuning dataset with diverse, realistic tasks based on
                real-world tables. <b>TableInstruct</b> boasts a collection of <b>14 datasets of 11 tasks</b> in total,
                which is curated from <b>1.24M</b> tables containing <b>2.6M</b> instances:
              <ul>
                <li>All data items are <b>unified into an instruction tuning manner</b> for further LLM training;</li>
                <li>All data items in <b>TableInstruct</b> are collected from real tables and real tasks;</li>
                <!-- <li>Each task in <b>TableInstruct</b> is a realistic task that can be applied to real-world scenarios.</li> -->
                <li><b>TableInstruct</b> provided both in-domain training tasks to empower the model with fundamental
                  table understanding abilities, and in & out-of-domain evaluation tasks to test the model’s
                  generalization and high-level reasoning ability.</li>
              </ul>
              </p>
              <p>
                <b>TableLlama</b> is a large generalist model for tables based on Llama 2 (7B) and LongLoRA, which can:
              <ul>
                <li><b>Support long input lengths (up to <code>8k</code>);</b></li>
                <li>Achieve <b>comparable or even better performance</b> than the SOTA on almost all of the in-domain
                  tasks;</li>
                <li>Achieve <b>6-48 absolute point gains</b> on 6 datasets compared with the base model, demonstrating
                  that TableInstruct can substantially enhance model generalizability.</li>
              </ul>
              <!-- is adopted to address the long context challenge. <b>TableLlama</b> is trained on <b>TableInstruct</b> and achieves comparable or even better performance than the SOTA on almost all of the in-domain tasks. For out-of-domain tasks, compared with the base model, TableLlama can achieve 6-48 absolute point gains on 6 datasets, which demonstrates that TableInstruct can substantially enhance model generalizability. -->

              <!-- Image carousel -->
              <section class="hero">
                <div class="hero-body">
                  <div class="container is-max-desktop">
                    <div class="columns is-centered">
                      <div class="column is-four-fifths">
                        <div class="item">
                          <!-- Your image here -->
                          <img src="static/images/overview1_yifei_1114-v13.png"
                            alt="An overview of TableInstruct and TableLlama">
                          <p>
                            <b>Figure 1</b>: An overview of TableInstruct and TableLlama. TableInstruct includes a wide
                            variety
                            of
                            realistic tables and tasks with instructions. We make the first step towards developing
                            open-source
                            generalist models for tables with TableInstruct and TableLlama.
                          </p>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <!-- End image carousel -->

              <!-- Paper abstract -->
              <section class="hero">
                <div class="hero-body">
                  <div class="container is-max-desktop">
                    <div class="columns is-centered">
                      <div class="column has-text-centered">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                          <p>
                            Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to
                            automatically interpret, augment, and query tables. Current methods often require
                            pretraining on tables or special model architecture design, are restricted to specific table
                            types, or have simplifying assumptions about tables and tasks. This paper makes the first
                            step towards developing open-source large
                            language models
                            (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct
                            TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction
                            tuning and
                            evaluating LLMs. We further develop the first open-source generalist model for tables,
                            TableLlama, by
                            fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment
                            under both
                            in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama
                            achieves com-
                            parable or better performance than the SOTA for each task, despite the latter often has
                            task-specific
                            design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the
                            base model,
                            showing that training on TableInstruct enhances the model’s generalizability. We will
                            open-source our
                            dataset and trained model to boost future work on developing open generalist models for
                            tables.
                          </p>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <!-- End paper abstract -->

              <p><i>Updates</i></p>
              <ul>
                <li>2023/11/20: We have released the <a
                    href="https://huggingface.co/datasets/osunlp/TableInstruct/">dataset</a>, <a
                    href="https://huggingface.co/osunlp/TableLlama/">model</a>, and <a
                    href="https://github.com/OSU-NLP-Group/TableLlama/">codebase</a> for the paper. Check it out!</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered is-fifths-fifths">
            <h2 class="title is-3">Our Dataset: TableInstruct</h2>
            <div class="content has-text-justified">
              <p>
                We construct <span><b>TableInstruct</b></span>, a comprehensive table-based instruction tuning dataset
                that covers a variety of real-world tables and realistic tasks. We include <b>14 datasets of 11
                  tasks</b> in total, with both <b>in-domain</b> and <b>out-of-domain evaluation settings</b>. Some
                examples can be found in the figure below:
              </p>
              <!-- Image carousel -->
              <section class="hero">
                <div class="hero-body">
                  <div class="container is-max-desktop">
                    <div class="columns is-centered">
                      <div class="column is-four-fifths">
                        <div class="item">
                          <!-- Your image here -->
                          <img src="static/images/TableLlama-fig2.png"
                            alt="The illustration of three exemplary tasks from TableInstruct">
                          <p>
                            <b>Figure 2</b>: Illustration of three exemplary tasks: (a) Column type annotation. This
                            task is to annotate
                            the selected column with the correct semantic types. (b) Row population. This task is to
                            populate rows
                            given table metadata and partial row entities. (c) Hierarchical table QA. For subfigures (a)
                            and (b), we
                            mark candidates with red color in the
                            "task instruction" part. The candidate set size can be hundreds to thousands in
                            TableInstruct.
                          </p>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </section>
              <!-- End image carousel -->
              <!-- <h4>Data Overview</h4> -->
              <!-- <p><b>TableInstruct</b> incorporates samples from 14 table-based datasets of 11 distinctive tasks.</p> -->
              <!-- <h4>Task Category</h4> -->
              
              <!-- <b>TableLlama</b> is an open-source LLM-based generalist model fine-tuned on <b>TableInstruct</b>.
              Experiments show that compared with the SOTA on each task that often has special pre-training or model
              architecture design for tables, TableLlama can achieve comparable or even better performance on almost all
              of the in-domain tasks. For out-of-domain tasks, compared with the base model, TableLlama can achieve 6-48
              absolute point gains on 6 datasets, which demonstrates that TableInstruct can substantially enhance model
              generalizability.
              TableInstruct is a dataset for developing and evaluating generalist agents for the web that can follow
              language instructions to complete complex tasks on any website. Mind2Web contains 2,350 tasks from 137
              websites spanning 31 domains that:
              Reflect diverse and practical use cases on the web.
              Provide challenging yet realistic environments with real-world websites.
              Test generalization ability across tasks and environments.
              In this work, we proposed <b>TableLlama</b> and <b>TableInstruct</b>,
              Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically
              interpret, augment, and query tables. Current methods often require pretraining on tables or special
              model architecture design, are restricted to specific table types, or have simplifying assumptions about
              tables and tasks. This paper makes the first step towards developing open-source large language models
              (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct
              TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and
              evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by
              fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both
              in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves com-
              parable or better performance than the SOTA for each task, despite the latter often has task-specific
              design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model,
              showing that training on TableInstruct enhances the model’s generalizability. We will open-source our
              dataset and trained model to boost future work on developing open generalist models for tables. -->
              </p>
            </div>
            
            <h2 class="title is-3">Data Statistics</h2>
            <div class="content has-text-justified">
              <p><b>TableInstruct</b> include a various kinds of table tasks to comprehensively represent different
                table-related applications in real-word scenario. These tasks are: table interpretation, table
                augmentation, question answering, fact verification, dialogue generation, and data-to-text. The table below shows the statistics of <b>TableInstruct</b>:</p>
              <div id="myTable_wrapper" class="dataTables_wrapper no-footer">
                <table id="myTable" class="dataTable no-footer" role="grid">
                  <thead>
                    <tr role="row">
                      <th style="vertical-align: bottom;">Task Category</th>
                      <th style="vertical-align: bottom;">Task Name</th>
                      <th style="vertical-align: bottom;">Dataset</th>
                      <th style="vertical-align: bottom;">In-domain</th>
                      <th style="vertical-align: bottom; text-align: middle;">Train (Table/Sample)</th>
                      <th style="vertical-align: bottom;">Test (Table/Sample)</th>
                      <th style="vertical-align: bottom;">min tokens</th>
                      <th style="vertical-align: bottom;">max tokens</th>
                      <th style="vertical-align: bottom;">median tokens</th>
                      <!-- <th colspan="3" style="vertical-align: bottom; text-align: middle;">Input Token Length</th> -->
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td rowspan="3" style="vertical-align: middle;">Table Interpretation</td>
                      <td style="vertical-align: middle;">Column Type Annotation</td>
                      <td rowspan="3" style="vertical-align: middle;">TURL</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">397K/628K</td>
                      <td style="vertical-align: middle;">1K/2K</td>
                      <td style="vertical-align: middle;">106</td>
                      <td style="vertical-align: middle;">8192</td>
                      <td style="vertical-align: middle;">2613</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: middle;">Relation Extraction</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">53K/63K</td>
                      <td style="vertical-align: middle;">1K/2K</td>
                      <td style="vertical-align: middle;">2602</td>
                      <td style="vertical-align: middle;">8192</td>
                      <td style="vertical-align: middle;">3219</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: middle;">Entity Linking</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">193K/1264K</td>
                      <td style="vertical-align: middle;">1K/2K</td>
                      <td style="vertical-align: middle;">299</td>
                      <td style="vertical-align: middle;">8192</td>
                      <td style="vertical-align: middle;">4667</td>
                    </tr>
                    <tr>
                      <td rowspan="2" style="vertical-align: middle;">Table Augmentation</td>
                      <td>Schema Augmentation</td>
                      <td rowspan="2" style="vertical-align: middle;">TURL</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">288K/288K</td>
                      <td style="vertical-align: middle;">4K/4K</td>
                      <td style="vertical-align: middle;">160</td>
                      <td style="vertical-align: middle;">1188</td>
                      <td style="vertical-align: middle;">215</td>
                    </tr>
                    <tr>
                      <td>Row Population</td>
                      <td>Yes</td>
                      <td>286K/286K</td>
                      <td>0.3K/0.3K</td>
                      <td>264</td>
                      <td>8192</td>
                      <td>1508</td>
                    </tr>
                    <tr>
                      <td rowspan="5" style="vertical-align: middle;">Question Answering</td>
                      <td style="vertical-align: middle;">Hierarchical Table QA</td>
                      <td style="vertical-align: middle;">HiTab</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">3K/7K</td>
                      <td style="vertical-align: middle;">1K/1K</td>
                      <td style="vertical-align: middle;">206</td>
                      <td style="vertical-align: middle;">5616</td>
                      <td style="vertical-align: middle;">978</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: middle;">Highlighted Cells QA</td>
                      <td style="vertical-align: middle;">FeTaQA</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">7K/7K</td>
                      <td style="vertical-align: middle;">2K/2K</td>
                      <td style="vertical-align: middle;">261</td>
                      <td style="vertical-align: middle;">5923</td>
                      <td style="vertical-align: middle;">740</td>
                    </tr>
                    <tr>
                      <td>Hybrid Table QA</td>
                      <td>HybridQA</td>
                      <td>No</td>
                      <td>-</td>
                      <td>3K/3K</td>
                      <td>248</td>
                      <td>2497</td>
                      <td>675</td>
                    </tr>
                    <tr>
                      <td>Table QA</td>
                      <td>WikiSQL</td>
                      <td>No</td>
                      <td>-</td>
                      <td>5K/16K</td>
                      <td>198</td>
                      <td>2091</td>
                      <td>575</td>
                    </tr>
                    <tr>
                      <td>Table QA</td>
                      <td>WikiTQ</td>
                      <td>No</td>
                      <td>-</td>
                      <td>0.4K/4K</td>
                      <td>263</td>
                      <td>2688</td>
                      <td>709</td>
                    </tr>
                    <tr>
                      <td rowspan="2" style="vertical-align: middle;">Fact Verification</td>
                      <td rowspan="2" style="vertical-align: middle;">Fact Verification</td>
                      <td style="vertical-align: middle;">TabFact</td>
                      <td style="vertical-align: middle;">Yes</td>
                      <td style="vertical-align: middle;">16K/92K</td>
                      <td style="vertical-align: middle;">2K/12K</td>
                      <td style="vertical-align: middle;">253</td>
                      <td style="vertical-align: middle;">4975</td>
                      <td style="vertical-align: middle;">630</td>
                    </tr>
                    <tr>
                      <td>FEVEROUS</td>
                      <td>No</td>
                      <td>-</td>
                      <td>4K/7K</td>
                      <td>247</td>
                      <td>8192</td>
                      <td>648</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: middle;">Dialogue Generation</td>
                      <td style="vertical-align: middle;">Table Grounded Dialogue Generation</td>
                      <td style="vertical-align: middle;">Fact Verification</td>
                      <td style="vertical-align: middle;">No</td>
                      <td style="vertical-align: middle;">-</td>
                      <td style="vertical-align: middle;">0.3K/0.8K</td>
                      <td style="vertical-align: middle;">187</td>
                      <td style="vertical-align: middle;">1103</td>
                      <td style="vertical-align: middle;">527</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: middle;">Data-to-Text</td>
                      <td style="vertical-align: middle;">Highlighted Cells Description</td>
                      <td style="vertical-align: middle;">ToTTo</td>
                      <td style="vertical-align: middle;">No</td>
                      <td style="vertical-align: middle;">-</td>
                      <td style="vertical-align: middle;">7K/8K</td>
                      <td style="vertical-align: middle;">152</td>
                      <td style="vertical-align: middle;">8192</td>
                      <td style="vertical-align: middle;">246</td>
                    </tr>
                    <!-- More rows following the same pattern -->
                  </tbody>
                </table>
                <!-- Additional HTML for pagination and search, if needed -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered is-fifths-fifths">
            <h2 class="title is-3">In-domain Evaluation</h2>
            
            <div class="content has-text-justified">
              We first evaluate <b>TableLlama</b> on 8 in-domain test sets. 
              <div id="myTable_wrapper" class="dataTables_wrapper no-footer">
                <table id="myTable" class="dataTable no-footer" role="grid">
                  <thead>
                    <tr role="row">
                      <th style="vertical-align: bottom;">Datasets</th>
                      <th style="vertical-align: bottom;">Metric</th>
                      <th style="vertical-align: bottom;">Base</th>
                      <th style="vertical-align: bottom;">TableLlama</th>
                      <th style="vertical-align: bottom;">SOTA</th>
                    </tr>
                  </thead>
                  <tbody>
                    </tr>
                    <td style="vertical-align: bottom;">Column Type Annotation</td>
                    <td style="vertical-align: bottom;">F1</td>
                    <td style="vertical-align: bottom;">3.01</td>
                    <td style="vertical-align: bottom;">94.39</td>
                    <td style="vertical-align: bottom;">94.54</td>
                    </tr>
                    <td style="vertical-align: bottom;">Relation Extraction</td>
                    <td style="vertical-align: bottom;">F1</td>
                    <td style="vertical-align: bottom;">0.96</td>
                    <td style="vertical-align: bottom;">91.95</td>
                    <td style="vertical-align: bottom;">94.91</td>
                    </tr>
                    <td style="vertical-align: bottom;">Entitly Linking</td>
                    <td style="vertical-align: bottom;">Accuracy</td>
                    <td style="vertical-align: bottom;">31.80</td>
                    <td style="vertical-align: bottom;">93.65</td>
                    <td style="vertical-align: bottom;">84.90</td>
                    </tr>
                    <td style="vertical-align: bottom;">Schema Augmentation</td>
                    <td style="vertical-align: bottom;">MAP</td>
                    <td style="vertical-align: bottom;">36.75</td>
                    <td style="vertical-align: bottom;">80.50</td>
                    <td style="vertical-align: bottom;">77.55</td>
                    </tr>
                    <td style="vertical-align: bottom;">Row Population</td>
                    <td style="vertical-align: bottom;">MAP</td>
                    <td style="vertical-align: bottom;">4.53</td>
                    <td style="vertical-align: bottom;">58.44</td>
                    <td style="vertical-align: bottom;">73.31</td>
                    </tr>
                    <td style="vertical-align: bottom;">HiTab</td>
                    <td style="vertical-align: bottom;">Exec Acc</td>
                    <td style="vertical-align: bottom;">14.96</td>
                    <td style="vertical-align: bottom;">64.71</td>
                    <td style="vertical-align: bottom;">47.00</td>
                    </tr>
                    <td style="vertical-align: bottom;">FeTaQA</td>
                    <td style="vertical-align: bottom;">BLEU</td>
                    <td style="vertical-align: bottom;">8.54</td>
                    <td style="vertical-align: bottom;">39.05</td>
                    <td style="vertical-align: bottom;">33.44</td>
                    </tr>
                    <td style="vertical-align: bottom;">TabFact</td>
                    <td style="vertical-align: bottom;">Accuracy</td>
                    <td style="vertical-align: bottom;">41.65</td>
                    <td style="vertical-align: bottom;">82.55</td>
                    <td style="vertical-align: bottom;">84.87</td>
                    </tr>
                    <!-- More rows following the same pattern -->
                  </tbody>
                </table>
                <!-- Additional HTML for pagination and search, if needed -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered is-fifths-fifths">
            <h2 class="title is-3">Out-of-domain Evaluation</h2>
            <div class="content has-text-justified">
              <div id="myTable_wrapper" class="dataTables_wrapper no-footer">
                <table id="myTable" class="dataTable no-footer" role="grid">
                  <thead>
                    <tr role="row">
                      <th style="vertical-align: bottom;">Datasets</th>
                      <th style="vertical-align: bottom;">Metric</th>
                      <th style="vertical-align: bottom;">Base</th>
                      <th style="vertical-align: bottom;">TableLlama</th>
                      <th style="vertical-align: bottom;">SOTA</th>
                      <th style="vertical-align: bottom;">\Delta_Base</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr role="row">
                      <td style="vertical-align: bottom;">FEVEROUS</td>
                      <td style="vertical-align: bottom;">Accuracy</td>
                      <td style="vertical-align: bottom;">23.66</td>
                      <td style="vertical-align: bottom;">72.30</td>
                      <td style="vertical-align: bottom;">82.40</td>
                      <td style="vertical-align: bottom;">+48.64</td>
                    </tr>
                    <tr role="row">
                      <td style="vertical-align: bottom;">HybridQA</td>
                      <td style="vertical-align: bottom;">Accuracy</td>
                      <td style="vertical-align: bottom;">20.72</td>
                      <td style="vertical-align: bottom;">27.61</td>
                      <td style="vertical-align: bottom;">63.40</td>
                      <td style="vertical-align: bottom;">+6.89</td>
                    </tr>
                    <tr role="row">
                      <td style="vertical-align: bottom;">KVRET</td>
                      <td style="vertical-align: bottom;">Micro F1</td>
                      <td style="vertical-align: bottom;">38.90</td>
                      <td style="vertical-align: bottom;">48.73</td>
                      <td style="vertical-align: bottom;">67.80</td>
                      <td style="vertical-align: bottom;">+9.83</td>
                    </tr>
                    <tr role="row">
                      <td style="vertical-align: bottom;">ToTTo</td>
                      <td style="vertical-align: bottom;">BLEU</td>
                      <td style="vertical-align: bottom;">10.39</td>
                      <td style="vertical-align: bottom;">20.77</td>
                      <td style="vertical-align: bottom;">48.95</td>
                      <td style="vertical-align: bottom;">+10.38</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: bottom;">WikiSQL</td>
                      <td style="vertical-align: bottom;">Accuracy</td>
                      <td style="vertical-align: bottom;">14.84</td>
                      <td style="vertical-align: bottom;">41.68</td>
                      <td style="vertical-align: bottom;">89.50</td>
                      <td style="vertical-align: bottom;">+26.84</td>
                    </tr>
                    <tr>
                      <td style="vertical-align: bottom;">WikiTQ</td>
                      <td style="vertical-align: bottom;">Accuracy</td>
                      <td style="vertical-align: bottom;">25.48</td>
                      <td style="vertical-align: bottom;">31.63</td>
                      <td style="vertical-align: bottom;">57.50</td>
                      <td style="vertical-align: bottom;">+6.15</td>
                    </tr>
                    <!-- More rows following the same pattern -->
                  </tbody>
                </table>
                <!-- Additional HTML for pagination and search, if needed -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Reference</h2>
      Please kindly cite our paper if you use our code, data, models or results:
      <br><br>
      <pre><code>@misc{zhang2023tablellama,
  title={TableLlama: Towards Open Large Generalist Models for Tables}, 
  author={Tianshu Zhang and Xiang Yue and Yifei Li and Huan Sun},
  year={2023},
  eprint={2311.09206},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>